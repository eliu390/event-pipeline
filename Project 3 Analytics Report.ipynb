{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "express-advantage",
   "metadata": {},
   "source": [
    "# W205 Project 3 Data Pipeline and Analytics Report\n",
    "## Eric Liu, Ian Dela Cruz, Luke Schanne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-increase",
   "metadata": {},
   "source": [
    "## Data Pipeline Files, Components and Setup\n",
    "\n",
    "The data pipe for this project consisted of the following components:\n",
    "\n",
    "### Pipeline Files\n",
    "- `docker-compose.yml`: details about the docker containers and environments that are needed to set-up and run the entire data pipeline.\n",
    "- `stream` directory: stores the files used to stream, filter, and write to hive. \n",
    "    - `[action].py` files: sets up the [action]'s spark streaming job.\n",
    "    - `template.py`: template file used to facilitate the setup of spark jobs.\n",
    "- `app` directory: stores the files that generate the events, dictate business logic, and write to sqllite tables.\n",
    "    - `game_api.py`: stores business logic for various types of actions (discussed below).\n",
    "    - `models.py`: write event data to sqllite tables.\n",
    "    - `events.py`: py script to generate a stream of random events.\n",
    "\n",
    "### Pipeline Components:\n",
    "- kafka topic: there is a single Kafka topic, `events` used in this data pipeline. From this topic, the various event types are filtered and written to hive tables using separate, action-specific spark streams.\n",
    "    - Kafka observer creation: docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "- sqllite tables: there are five tables that are created and used to store the data generated by the game API for reference for some light business logic application.\n",
    "    - swords\n",
    "    - transactions\n",
    "    - guilds\n",
    "    - players\n",
    "    - guild_interactions\n",
    "- spark jobs: there are action-specific spark jobs that filter the events into their respective hive tables.\n",
    "- hive tables: there are five, action specific hive tables that are created for querying by Presto:\n",
    "    - swords\n",
    "    - sword_transactions\n",
    "    - guilds\n",
    "    - players\n",
    "    - guild_membership"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-pension",
   "metadata": {},
   "source": [
    "## Game API Details\n",
    "\n",
    "Our group decided to expand on the initial project prompt by adding to the available actions within the game API and the data that would be generated with each transaction. In addition to the 'buy sword' action, we added 'add player', 'add guild', 'join guild', and 'add sword'. We felt that this would be an appropriate level of complexity to test our data pipeline creation, coding, and querying skills. As you will observe within the code, a few of these actions may also have additional parameters that need to be specified upon creation that we felt made sense to, given a standard understanding of typical game dynamics. These actions may also return additional data, such as data about interactions between players, between players and objects, as well as temporal data, providing us with a rich dataset from which we would then flex our Presto querying skills.\n",
    "\n",
    "The noteworthy implications of this added complexity that we would like to highlight are as follows:\n",
    "\n",
    "- Base event data written to the final hive tables are captured in a single string that is then parsed using regex, depending on the analytics question. See Basic Event Analysis section below.\n",
    "- Creation of persistent sqllite.db file, which allows for querying in applying business logic prior to writing to hive.\n",
    "- The resulting business logic (for example, how to vlaidate a transaction or guild interaction) became tricky to implement. The group is sure there are edge cases where the code will fail. Here, we assumed correcting for every possile case was outside the scope of the project, and instead we focus on ensuring a viable MVP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-instrumentation",
   "metadata": {},
   "source": [
    "## Basic Event Generation\n",
    "\n",
    "We can generate events manually. Depending on the action, there may be parameters that will need to be defined. Examples of how to run each action from the terminal are provided below:\n",
    "\n",
    "- add a guild: docker-compose exec mids curl \"http://localhost:5000/add_guild?name=jokers\"\n",
    "- add a player: docker-compose exec mids curl \"http://localhost:5000/add_playter?name=batman?money=999999\"\n",
    "- join a guild: docker-compose exec mids curl \"http://localhost:5000/join_guild?player_id=1&guild_id=1&join=1\"\n",
    "- add a sword: docker-compose exec mids curl \"http://localhost:5000/add_sword?cost=100\"\n",
    "- buy a sword: docker-compose exec mids curl \"http://localhost:5000/purchase_sword?buyer_id=1&sword_id=1\"\n",
    "\n",
    "We can also generate events automatically, using the `events.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-comfort",
   "metadata": {},
   "source": [
    "## Basic Event Analysis: Sample Preso Queries to answer simple business questions\n",
    "\n",
    "Presto can be started from the terminal using the below code:\n",
    "\n",
    "- docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "\n",
    "From here, a variety of business questions can be explored. We provide a few examples of Presto queries that may be of business value to explore.\n",
    "\n",
    "Some notes:\n",
    "\n",
    "- The logic of these queries (especially those dealing with transaction data) may not robustly account for faulty business logic. The group felt that the proper place to enforce business logic would be further upstream in the application pipeline (not in the queries), and that the vast majority of this sort of robustness was outside the scope of this project.\n",
    "\n",
    "- Because of how the event data are captured in the expanded game, some initial regex will need to be performed in order to isolate the paramters of interest. The group felt that it was beyond the scope of this project to create custom tables to service each of the numberous types of business questions that we could potentially explore.\n",
    "\n",
    "- Finally, while the results of each query are not explicitly shown in this ipynb, we will run through some of these queries during our live demo, to demonstrate how they are used. \n",
    "\n",
    "\n",
    "### Who are my players?\n",
    "SELECT\n",
    "    distinct regexp_extract(event_body,'(?<=name\\\": \\\")(.*?)(?=\"}})') as distinct_players\n",
    "FROM\n",
    "    players;\n",
    "\n",
    "\n",
    "### How many guilds are there?\n",
    "SELECT\n",
    "    count(distinct regexp_extract(event_body,'(?<=name\\\": \\\")(.*?)(?=\"}})') ) as number_of_guilds\n",
    "FROM\n",
    "    guilds;\n",
    "\n",
    "\n",
    "### Which sword is exhanged the most?\n",
    "SELECT\n",
    "    regexp_extract(event_body,'(?<=sword_id\\\": )(.*?)(?=, \\\"trans)') as sword_id, \n",
    "    count( regexp_extract(event_body,'(?<=sword_id\\\": )(.*?)(?=, \\\"trans)') ) as count_of_transactions\n",
    "FROM\n",
    "    sword_transactions\n",
    "GROUP BY\n",
    "    regexp_extract(event_body,'(?<=sword_id\\\": )(.*?)(?=, \\\"trans)')\n",
    "ORDER BY \n",
    "    count( regexp_extract(event_body,'(?<=sword_id\\\": )(.*?)(?=, \\\"trans)') ) DESC;\n",
    "\n",
    "\n",
    "### Which guild has the longest membership?\n",
    "\n",
    "SELECT c.guild_id, c.player_id, sum(duration) as membership_length\n",
    "FROM (\n",
    "    SELECT b.guild_id, b.player_id,\n",
    "        CASE\n",
    "            WHEN b.action = 'true' THEN date_diff('minute', b.timestamp, current_timestamp)\n",
    "            WHEN b.action = 'false' THEN -date_diff('minute', b.timestamp, current_timestamp)\n",
    "            END as duration\n",
    "    FROM(\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT * , rank() over (\n",
    "                PARTITION BY a.guild_id, a.player_id\n",
    "                ORDER BY a.timestamp DESC) as rank\n",
    "            FROM (\n",
    "                SELECT\n",
    "                    regexp_extract(event_body,'(?<=guild_id\\\": )(.*?)(?=}})') as guild_id,\n",
    "                    regexp_extract(event_body,'(?<=player_id\\\": )(.*?)(?=, \\\"timestamp)') as player_id, \n",
    "                    regexp_extract(event_body,'(?<=join\\\": )(.*?)(?=, \\\"player)') as action,\n",
    "                    date_add('year', 100, date_parse(regexp_extract(event_body,'(?<=timestamp\\\": \\\")(.*?)(?=\\\", \\\"guild)'), '%m/%d/%Y %T')) as timestamp\n",
    "                FROM\n",
    "                    guild_membership) a )\n",
    "        WHERE rank = 1 OR rank = 2 ) b ) c\n",
    "GROUP BY\n",
    "    c.guild_id, c.player_id\n",
    "ORDER BY \n",
    "    sum(duration) DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-updating",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
